{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28773b44",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Multi-Instance Learning </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab44474",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The notebook implements multi-instance learning (MIL) using the DeliciousMILDataset, which contains text documents and their associated binary labels. Two experiments are conducted, one at the document level (vectorized text input) and another at the sentence level (clusters-based input), using multiple classifiers and hyperparameter settings. The second approach uses k-means clustering to extract features from bags of words and sentences, replacing the standard feature extraction method in scikit-learn. This approach allows for more interpretable and customizable feature extraction, with the added benefit of scalability to large datasets. Finally the results reported in terms of accuracy, precision, and recall.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7359c",
   "metadata": {},
   "source": [
    "## Generals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83323fb4",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Packages import and system configurations. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42994619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea2dfeb",
   "metadata": {},
   "source": [
    "## Load & Preproces Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85eadbd",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "The classes aim to read and process document data from DeliciousMILDataset, and build vectors for documents and sentences. Using the following class we can represent the dataset in both approaches requested by the pronunciation. \n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Define Document class with empty sentence, label, and vector lists.</li>\n",
    "<li>Define DeliciousMILDataset class with vocab, train/test documents, labels, and sentence labels.</li>\n",
    "<li>Define two methods, get_data_on_document_level and get_data_on_sentence_level, which build vectors for the documents or sentences.</li>\n",
    "<li>Read data, labels, vocabulary, and sentence labels from corresponding files.</li>\n",
    "<li>Build vectors for each document or sentence by iterating over sentences and their values.</li>\n",
    "<li>If sparse=True, use csr_matrix to represent the vectors in a compressed form.</li>\n",
    "</ol>\n",
    "<br>\n",
    "In more details, the class contains the bellow functions:\n",
    "<br>\n",
    "<br>\n",
    "<li><b>_read_data():</b> Reads the data file and creates a list of Document objects, each containing a list of Document objects representing sentences.</li>\n",
    "<li><b>_read_labels():</b> Reads the label file and creates a list of labels for each document.</li>\n",
    "<li><b>_read_vocabulary():</b> Reads the vocabulary file and creates a dictionary of words and their corresponding indices.</li>\n",
    "<li><b>_read_test_sentence_labels():</b> Reads the labeled test sentences file and creates a dictionary mapping each sentence to its labels.</li>\n",
    "<li><b>_build_vectors():</b> Builds vectors for each document and its sentences by calling _buld_vector for each document.</li>\n",
    "<li><b>_buld_vector():</b> Builds a vector for a given document by summing the vectors of its sentences. The resulting vector is stored in document.vector. If keep_sentence_vector=True, the vector for each sentence is also stored in sent.vector.</li>\n",
    "<li><b>get_data_on_document_level():</b> Builds document-level vocabulary vectors for both the training and test datasets and returns the resulting Document objects. Vectros have the lenght of the global vocabulary and represets the number of each word existance.</li>\n",
    "<li><b>get_data_on_sentence_level():</b> Builds sentence-level vocabulary vectors for both the training and test datasets and returns the resulting Document objects. Vectros have the lenght of the global vocabulary and represets the number of each word existance.</li>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d575f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentences = [] # Document objects\n",
    "        self.labels = []\n",
    "        self.vector = []\n",
    "\n",
    "class DeliciousMILDataset(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_data=\"./data/train-data.dat\", test_data=\"./data/test-data.dat\",\n",
    "                 train_labels=\"./data/train-label.dat\", test_labels=\"./data/test-label.dat\",\n",
    "                 labeled_test_sentences=\"./data/labeled_test_sentences.dat\", vocab_file=\"./data/vocabs.txt\"):\n",
    "\n",
    "        self._vocab = self._read_vocabulary(vocab_file)\n",
    "        self._train_documents = self._read_data(train_data)\n",
    "        self._test_documents = self._read_data(test_data)\n",
    "        self._train_labels = self._read_labels(train_labels)\n",
    "        self._test_labels = self._read_labels(test_labels)\n",
    "        self._test_sentence_labels = self._read_test_sentence_labels(labeled_test_sentences)\n",
    "\n",
    "        \n",
    "    def get_data_on_document_level(self, sparse):\n",
    "        self._build_vectors\\\n",
    "            (self._train_documents, self._train_labels, len(self._vocab), keep_sentence_vector=False, sparse=sparse)\n",
    "        self._build_vectors\\\n",
    "            (self._test_documents, self._test_labels, len(self._vocab), keep_sentence_vector=False, sparse=sparse)\n",
    "\n",
    "        return self._train_documents, \\\n",
    "               self._test_documents\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \n",
    "        self._read_vocabulary()\n",
    "            \n",
    "        return self._vocab\n",
    "    \n",
    "    def get_data_on_sentence_level(self, sparse):\n",
    "        self._build_vectors\\\n",
    "            (self._train_documents, self._train_labels, len(self._vocab), keep_sentence_vector=True, sparse=sparse)\n",
    "        self._build_vectors\\\n",
    "            (self._test_documents, self._test_labels, len(self._vocab), keep_sentence_vector=True, sparse=sparse)\n",
    "\n",
    "        return self._train_documents, \\\n",
    "               self._test_documents\n",
    "\n",
    "    \n",
    "    def _read_data(self, data_file):\n",
    "        documents = []\n",
    "\n",
    "        with open(data_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.split()\n",
    "\n",
    "                document = Document()\n",
    "\n",
    "                # First token is number of sentences, ignore\n",
    "                tokens.pop(0)\n",
    "\n",
    "                while len(tokens) > 0:\n",
    "                    sentence_num_token = tokens.pop(0)\n",
    "\n",
    "                    num_sentences = int(sentence_num_token[1:-1])\n",
    "\n",
    "                    sentence = Document()\n",
    "\n",
    "                    for i in range(num_sentences):\n",
    "                        sentence.vector.append(int(tokens.pop(0)))\n",
    "\n",
    "                    document.sentences.append(sentence)\n",
    "\n",
    "                documents.append(document)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    \n",
    "    def _read_labels(self, label_file):\n",
    "        labels = []\n",
    "\n",
    "        with open(label_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "\n",
    "                labels.append([int(v) for v in line.split()])\n",
    "\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def _read_vocabulary(self, vocab_file):\n",
    "        word_dict = {}\n",
    "\n",
    "        with open(vocab_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.split(\",\")\n",
    "                \n",
    "                word_dict[tokens[0].strip()] = int(tokens[1].strip())\n",
    "                \n",
    "        return word_dict\n",
    "\n",
    "    \n",
    "    def _read_test_sentence_labels(self, label_file):\n",
    "        doc_sent_labels_dict = {}\n",
    "\n",
    "        with open(label_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.split()\n",
    "\n",
    "                doc = tokens.pop(0)\n",
    "                sent = tokens.pop(0)\n",
    "\n",
    "                if doc not in doc_sent_labels_dict:\n",
    "                    doc_sent_labels_dict[doc] = {}\n",
    "\n",
    "                doc_sent_labels_dict[doc][sent] = [int(v) for v in tokens]\n",
    "\n",
    "        return doc_sent_labels_dict\n",
    "\n",
    "    \n",
    "    def _build_vectors(self, documents, labels, vocabulary_length, keep_sentence_vector=True, sparse=True):\n",
    "        for doc, doc_labels in zip(documents, labels):\n",
    "            self._buld_vector(doc, doc_labels, vocabulary_length, keep_sentence_vector)\n",
    "\n",
    "            \n",
    "    def _buld_vector(self, document, doc_labels, vocabulary_length, keep_sentence_vector=True, sparse=True):\n",
    "        vec = np.zeros(vocabulary_length)\n",
    "\n",
    "        for sent in document.sentences:\n",
    "            sent_vec = np.zeros(vocabulary_length)\n",
    "\n",
    "            for val in sent.vector:\n",
    "                sent_vec[val] += 1\n",
    "\n",
    "            vec += sent_vec\n",
    "            if keep_sentence_vector:\n",
    "                if sparse:\n",
    "                    sent.vector = scipy.sparse.csr_matrix(sent_vec)\n",
    "                else:\n",
    "                    sent.vector = sent_vec\n",
    "\n",
    "        if sparse:\n",
    "            document.vector = scipy.sparse.csr_matrix(vec)\n",
    "            document.labels = scipy.sparse.csr_matrix(doc_labels)\n",
    "        else:\n",
    "            document.vector = vec\n",
    "            document.labels = doc_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa5282",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the function bellow is to load and prepare the DeliciousMILDataset for classification.\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Initialize a DeliciousMILDataset object.</li>\n",
    "<li>If sentence_level is True, extract the dataset on the sentence level, else extract on document level.</li>\n",
    "<li>Calculate the most frequent label class across all documents in the training set.</li>\n",
    "<li>Extract the labels of each document and assign the label of the most frequent class to it. Return the train and test datasets and their corresponding labels.</li>\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7d3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(sentence_level):\n",
    "    d = DeliciousMILDataset()\n",
    "    \n",
    "    if sentence_level:\n",
    "        train_docs, test_docs = d.get_data_on_sentence_level(sparse=False)\n",
    "    else:\n",
    "        train_docs, test_docs = d.get_data_on_document_level(sparse=False)\n",
    "    \n",
    "    label_sum = np.zeros(train_docs[0].labels.shape[1])\n",
    "    for doc in train_docs:\n",
    "        label_sum += doc.labels\n",
    "        \n",
    "    most_freq_class = np.argmax(label_sum)\n",
    "    train_labels = np.asarray([doc.labels[0, most_freq_class] for doc in train_docs])\n",
    "    test_labels = np.asarray([doc.labels[0, most_freq_class] for doc in test_docs])\n",
    "    return train_docs, test_docs, train_labels, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068b2a8",
   "metadata": {},
   "source": [
    "## Vectorize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397d6df",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the function bellow is to train a KMeans clustering model on multi-instance data and return the fitted model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01afa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multi_instance_kmeans(train_x, paramset):\n",
    "    base_estimator = KMeans(paramset['k'])\n",
    "    base_estimator.fit(train_x)\n",
    "    return base_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7f632",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the function bellow is to transform the input data into a fixed-dimensional representation using the previously fitted KMeans model. By doing this, the bags are transformed into vectors of cluster frequencies by computing the distance of each instance in the bag to each of the cluster centroids, and summing the distances for each cluster. Function returns a list of vectors, where each vector represents a bag of instances. The function can optionally use distances between instances and cluster centers instead of binary cluster memberships.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3c4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_multi_instance_kmeans(x, base_estimator, paramset):\n",
    "    vectors = []\n",
    "\n",
    "    for test_bag in x:\n",
    "        vec = np.zeros(paramset['k'])\n",
    "\n",
    "        if paramset['use_distances']:\n",
    "            for sent_vec in base_estimator.transform(test_bag):\n",
    "                vec += sent_vec\n",
    "            vec = normalize(vec.reshape(1, -1))[0]\n",
    "        else:\n",
    "            for c in base_estimator.predict(test_bag):\n",
    "                vec[c] = 1\n",
    "\n",
    "        vectors.append(vec)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a19cc",
   "metadata": {},
   "source": [
    "## Sentence Level Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bee4f8",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the function bellow is to to conduct experiments on a sentence-level dataset using different classifiers (cluster-based input).\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Get the dataset for sentence-level classification.</li>\n",
    "<li>Extract the sentence-level vectors from the training and test documents by iterating over the sentences in each document, and stacking the sentence vectors vertically.</li>\n",
    "<li>For each set of parameters in params, train a multi-instance k-means vectorizer.</li>\n",
    "<li>Transform the training and test sentence vectors into bag-level representations using the trained vectorizer.</li>\n",
    "<li>Train each classifier on the transformed training data and labels using the fit function.</li>\n",
    "<li>Make predictions on the transformed test data using the predict function.</li>\n",
    "<li>Print the results, including the model name, parameter settings, and evaluation metrics (accuracy, precision, and recall) using the print function.</li>\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb4e918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments_sentence_level(classifiers,params):\n",
    "    train_docs, test_docs, train_labels, test_labels = get_dataset(sentence_level = True)\n",
    "\n",
    "    train_sentences = []\n",
    "    for doc in train_docs:\n",
    "        train_sentences.extend([sent.vector for sent in doc.sentences])\n",
    "    train_sentences = scipy.sparse.vstack(train_sentences)\n",
    "    \n",
    "    train_sentence_vectors = []\n",
    "    for doc in train_docs:\n",
    "        train_sentence_vectors.append(scipy.sparse.vstack([sent.vector for sent in doc.sentences]))\n",
    "    test_sentence_vectors = []\n",
    "    for doc in test_docs:\n",
    "        test_sentence_vectors.append(scipy.sparse.vstack([sent.vector for sent in doc.sentences]))\n",
    "\n",
    "    i=0\n",
    "    for param_set in params:\n",
    "        i += 1\n",
    "        print('\\nExperiment :',i)\n",
    "        vectorizer = fit_multi_instance_kmeans(train_sentences, param_set)\n",
    "        \n",
    "        transformed_train_vec = np.asarray(transform_multi_instance_kmeans(train_sentence_vectors, vectorizer, param_set))\n",
    "        transformed_test_vec = np.asarray(transform_multi_instance_kmeans(test_sentence_vectors, vectorizer, param_set))\n",
    "        \n",
    "        for classifier_name in classifiers:\n",
    "            classifier = classifiers[classifier_name]()\n",
    "            classifier.fit(transformed_train_vec, train_labels)\n",
    "            predictions = classifier.predict(transformed_test_vec)\n",
    "\n",
    "            print(\"Model:{}, K:{}, Use distances:{}, Accuracy:{}, Precision:{}, Recall:{}\"\n",
    "                  .format(classifier_name,\n",
    "                          *param_set.values(),\n",
    "                          round(accuracy_score(test_labels, predictions),5),\n",
    "                          round(precision_score(test_labels, predictions),5),\n",
    "                          round(recall_score(test_labels, predictions),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db88f5",
   "metadata": {},
   "source": [
    "## Document Level Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1208a",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The aim of the function bellow is to to conduct experiments on a document-level dataset using different classifiers (vocabulay existance-based input vector).\n",
    "<br>\n",
    "<br>\n",
    "The function performs the following steps:\n",
    "<ol>\n",
    "<li>Get the dataset on document level.</li>\n",
    "<li>Train and test different classifiers using accuracy, precision, and recall scores.</li>\n",
    "<li>Print the results for each classifier and parameter set.</li>\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e781648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments_document_level(classifiers, params):  \n",
    "    train_docs, test_docs, train_labels, test_labels = get_dataset(sentence_level=False)\n",
    "    \n",
    "    train_vec = scipy.sparse.vstack([doc.vector for doc in train_docs])\n",
    "    test_vec = scipy.sparse.vstack([doc.vector for doc in test_docs])\n",
    "    i=0\n",
    "    for classifier_name in classifiers:\n",
    "        classifier = classifiers[classifier_name]()\n",
    "        classifier.fit(train_vec, train_labels)\n",
    "        predictions = classifier.predict(test_vec)\n",
    "\n",
    "        print(\"Model:{}, Accuracy:{}, Precision:{}, Recall:{}\"\n",
    "              .format(classifier_name,\n",
    "                      round(accuracy_score(test_labels, predictions),5),\n",
    "                      round(precision_score(test_labels, predictions),5),\n",
    "                      round(recall_score(test_labels, predictions),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49742f5",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e0e05",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Global Envariables.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e75bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "        \"RandomForests\": RandomForestClassifier,\n",
    "        \"LinearSVC\": LinearSVC,\n",
    "        \"XGBoost\": xgb.XGBClassifier\n",
    "}\n",
    "\n",
    "params = [{\"k\": 5, \"use_distances\": False},\n",
    "        {\"k\": 5, \"use_distances\": True},\n",
    "        {\"k\": 10, \"use_distances\": False},\n",
    "        {\"k\": 10, \"use_distances\": True},\n",
    "        {\"k\": 25, \"use_distances\": False},\n",
    "        {\"k\": 25, \"use_distances\": True},\n",
    "        {\"k\": 50, \"use_distances\": False},\n",
    "        {\"k\": 50, \"use_distances\": True},]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8d1e8",
   "metadata": {},
   "source": [
    "### Approach 1: Sentence Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45caf896",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Cluster the proposals of the training set and represent each document by the clusters to which its proposals belong.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f590c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment : 1\n",
      "Model:RandomForests, K:5, Use distances:False, Accuracy:0.60984, Precision:0.54762, Recall:0.01476\n",
      "Model:LinearSVC, K:5, Use distances:False, Accuracy:0.60909, Precision:0.66667, Recall:0.00128\n",
      "Model:XGBoost, K:5, Use distances:False, Accuracy:0.60984, Precision:0.54762, Recall:0.01476\n",
      "\n",
      "Experiment : 2\n",
      "Model:RandomForests, K:5, Use distances:True, Accuracy:0.57771, Precision:0.43501, Recall:0.26637\n",
      "Model:LinearSVC, K:5, Use distances:True, Accuracy:0.60884, Precision:0.5, Recall:0.00064\n",
      "Model:XGBoost, K:5, Use distances:True, Accuracy:0.57971, Precision:0.43224, Recall:0.23748\n",
      "\n",
      "Experiment : 3\n",
      "Model:RandomForests, K:10, Use distances:False, Accuracy:0.6003, Precision:0.46816, Recall:0.16046\n",
      "Model:LinearSVC, K:10, Use distances:False, Accuracy:0.6116, Precision:0.51792, Recall:0.10205\n",
      "Model:XGBoost, K:10, Use distances:False, Accuracy:0.60658, Precision:0.49072, Recall:0.15276\n",
      "\n",
      "Experiment : 4\n",
      "Model:RandomForests, K:10, Use distances:True, Accuracy:0.60331, Precision:0.48632, Recall:0.25096\n",
      "Model:LinearSVC, K:10, Use distances:True, Accuracy:0.60859, Precision:0.0, Recall:0.0\n",
      "Model:XGBoost, K:10, Use distances:True, Accuracy:0.59126, Precision:0.46641, Recall:0.31194\n",
      "\n",
      "Experiment : 5\n",
      "Model:RandomForests, K:25, Use distances:False, Accuracy:0.58047, Precision:0.44229, Recall:0.27792\n",
      "Model:LinearSVC, K:25, Use distances:False, Accuracy:0.61386, Precision:0.52488, Recall:0.13543\n",
      "Model:XGBoost, K:25, Use distances:False, Accuracy:0.5998, Precision:0.47897, Recall:0.26316\n",
      "\n",
      "Experiment : 6\n",
      "Model:RandomForests, K:25, Use distances:True, Accuracy:0.61185, Precision:0.50763, Recall:0.2561\n",
      "Model:LinearSVC, K:25, Use distances:True, Accuracy:0.60884, Precision:0.0, Recall:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/full_ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:XGBoost, K:25, Use distances:True, Accuracy:0.60532, Precision:0.49354, Recall:0.34339\n",
      "\n",
      "Experiment : 7\n",
      "Model:RandomForests, K:50, Use distances:False, Accuracy:0.60457, Precision:0.49079, Recall:0.29076\n",
      "Model:LinearSVC, K:50, Use distances:False, Accuracy:0.61562, Precision:0.52139, Recall:0.21117\n",
      "Model:XGBoost, K:50, Use distances:False, Accuracy:0.59729, Precision:0.47767, Recall:0.31579\n",
      "\n",
      "Experiment : 8\n",
      "Model:RandomForests, K:50, Use distances:True, Accuracy:0.62315, Precision:0.53696, Recall:0.26573\n",
      "Model:LinearSVC, K:50, Use distances:True, Accuracy:0.60884, Precision:0.0, Recall:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/full_ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:XGBoost, K:50, Use distances:True, Accuracy:0.61813, Precision:0.51683, Recall:0.36457\n"
     ]
    }
   ],
   "source": [
    "experiments_sentence_level(classifiers, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14465f",
   "metadata": {},
   "source": [
    "### Approach 2: Document Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ebca7",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Represent each document by all its propositions represent dataset without using clusters but with a common vocabulary existance-based representation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebf4970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:RandomForests, Accuracy:0.6578, Precision:0.64931, Recall:0.27214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/full_ml/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:LinearSVC, Accuracy:0.61788, Precision:0.51135, Recall:0.52054\n",
      "Model:XGBoost, Accuracy:0.66206, Precision:0.60794, Recall:0.38318\n"
     ]
    }
   ],
   "source": [
    "experiments_document_level(classifiers, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d26165",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f3610",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "<b>Comparison:</b>\n",
    "<br>\n",
    "The document-level approach (vector-based input) outperforms the sentence-level approach (cluster-based input) in terms of accuracy, precision, and recall, as seen in most experiments. Comparing the two approaches, we can see that Approach 2 generally produces higher performance than Approach 1, with higher accuracy, precision, and recall for all three models. \n",
    "\n",
    "<b>Apprach 1 (K-Means Hyper-Parameters):</b>\n",
    "<br>\n",
    "The choice of k and whether to use distances has a significant impact on the performance of the models. Using distances generally leads to lower performance than not using distances, possibly because the distances contain noise and irrelevant information.\n",
    "The optimal value of k depends on the model and the use of distances. In general, smaller values of k tend to lead to higher accuracy and precision, but lower recall.For Random Forests and XGBoost, using k=5 without distances tends to produce the best performance across all three metrics. For LinearSVC, using k=10 without distances leads to the best accuracy and precision, but with lower recall.\n",
    "    \n",
    "It's important to note that the performance of the models is generally not very high, with accuracy and precision around 60% and recall around 25-30%. This suggests that there may be other factors beyond the choice of k and use of distances that are limiting the performance of the models.\n",
    "    \n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full_ml",
   "language": "python",
   "name": "full_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
